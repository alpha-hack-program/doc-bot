---
apiVersion: batch/v1
kind: Job
metadata:
  name: job-{{ .Values.model.name }}-model-setup
  namespace: "{{ .Values.dataScienceProjectNamespace }}"
  annotations:
    job.kubernetes.io/disabled: "true"
spec:
  selector: {}
  template:
    spec:
      containers:
        - args:
            - -ec
            - |-
              echo -n 'Waiting for hf-creds secret'
              while ! oc get secret hf-creds 2>/dev/null; do
                echo -n .
                sleep 5
              done; echo
              
              HF_USERNAME=$(oc get secret hf-creds -o jsonpath='{.data.HF_USERNAME}' | base64 -d)
              HF_TOKEN=$(oc get secret hf-creds -o jsonpath='{.data.HF_TOKEN}' | base64 -d)
              MODEL_ROOT="{{ .Values.model.root }}"
              MODEL_ID="{{ .Values.model.id }}"

              cd /mnt/models

              # Check if the directory exists
              if [ -d "${MODEL_ID}" ]; then
                  echo "Directory '${MODEL_ID}' already exists. Pulling latest changes..."

                  # Navigate into the directory
                  cd "${MODEL_ID}"

                  # Pull latest changes from remote repository
                  git pull origin main
              else
                echo "Directory '${MODEL_ID}' does not exist. Cloning repository..."
              
                git clone https://${HF_USERNAME}:${HF_TOKEN}@huggingface.co/${MODEL_ROOT}/${MODEL_ID}
              fi

              MINIO_ROOT_USER={{ .Values.pipelinesConnection.awsAccessKeyId }}
              MINIO_ROOT_PASSWORD={{ .Values.pipelinesConnection.awsSecretAccessKey }}
              MINIO_HOST={{ .Values.pipelinesConnection.scheme }}://{{ .Values.pipelinesConnection.awsS3Endpoint }}

              AWS_S3_BUCKET={{ .Values.modelConnection.awsS3Bucket }}
              AWS_ACCESS_KEY_ID={{ .Values.modelConnection.awsAccessKeyId }}
              AWS_SECRET_ACCESS_KEY={{ .Values.modelConnection.awsSecretAccessKey }}
              AWS_DEFAULT_REGION={{ .Values.modelConnection.awsDefaultRegion }}
              AWS_S3_ENDPOINT={{ .Values.modelConnection.awsS3Endpoint }}
              AWS_S3_CUSTOM_DOMAIN=${AWS_S3_ENDPOINT}
              AWS_S3_USE_PATH_STYLE=1

              # Configure the AWS CLI
              aws configure set aws_access_key_id ${AWS_ACCESS_KEY_ID}
              aws configure set aws_secret_access_key ${AWS_SECRET_ACCESS_KEY}
              aws configure set default.region ${AWS_DEFAULT_REGION}

              # Create a bucket
              aws s3api create-bucket --bucket ${AWS_S3_BUCKET} --endpoint-url "{{ .Values.modelConnection.scheme }}://${AWS_S3_ENDPOINT}" 

              # Upload the model to files folder in the bucket
              aws s3 sync ${MODEL_ID} s3://${AWS_S3_BUCKET}/${MODEL_ROOT}/${MODEL_ID}/ --endpoint-url "https://${AWS_S3_ENDPOINT}" 
          command:
            - /bin/bash
          image: {{ .Values.setup.image }}
          imagePullPolicy: Always
          name: create-ds-connections
          volumeMounts:
            - name: models-volume
              mountPath: /mnt/models
      restartPolicy: Never
      serviceAccount: setup-job
      serviceAccountName: setup-job
      volumes:
        - name: models-volume
          persistentVolumeClaim:
            claimName: models-pvc
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: models-pvc
  namespace: "{{ .Values.dataScienceProjectNamespace }}"
  labels:
    app: models-pvc
  annotations:
    argocd.argoproj.io/sync-wave: "1"
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi