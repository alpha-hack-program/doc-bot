dataScienceProjectDisplayName: vllm-mistral-7b
dataScienceProjectNamespace: vllm-mistral-7b

instanceName: vllm-mistral-7b

createNamespace: true

model:
  root: mistralai
  id: Mistral-7B-Instruct-v0.2
  name: mistral-7b
  displayName: Mistral 7b
  maxReplicas: 1
  format: vLLM
  maxModelLen: '6144'
  apiProtocol: REST
  runtime:
    templateName: vllm-serving-template
    templateDisplayName: vLLM Serving Template
    image: quay.io/modh/vllm@sha256:c86ff1e89c86bc9821b75d7f2bbc170b3c13e3ccf538bf543b1110f23e056316
    resources:
      limits:
        cpu: '8'
        memory: 24Gi
      requests:
        cpu: '6'
        memory: 24Gi
  accelerator:
    max: '1'
    min: '1'
    productName: NVIDIA-A10G
  connection:
    name: llm
  volumes:
    shm:
      sizeLimit: 2Gi
    
modelConnection:
  createSecret: true
  name: llm
  displayName: llm
  type: s3
  scheme: http
  awsAccessKeyId: minio
  awsSecretAccessKey: minio123
  awsDefaultRegion: none
  awsS3Bucket: models
  awsS3Endpoint: minio.ic-shared-minio.svc:9000

setup:
  image: quay.io/atarazana/hf-cli:latest