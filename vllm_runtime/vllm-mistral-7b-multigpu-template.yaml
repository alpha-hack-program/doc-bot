apiVersion: template.openshift.io/v1
kind: Template
metadata:
  annotations:
    description: vLLM is blah blah.
    opendatahub.io/apiProtocol: REST
    opendatahub.io/modelServingSupport: '["single"]'
  labels:
    opendatahub.io/dashboard: "true"
  name: vllm-mistral-7b-2x-t4-serving-template
  namespace: redhat-ods-applications
objects:
- apiVersion: serving.kserve.io/v1alpha1
  kind: ServingRuntime
  metadata:
    annotations:
      opendatahub.io/accelerator-name: migrated-gpu
      opendatahub.io/apiProtocol: REST
      opendatahub.io/template-display-name: vLLM Mistral 7B 2xT4
      opendatahub.io/template-name: vllm-mistral-7b-2x-t4
      openshift.io/display-name: vLLM Mistral 7b 2x T4
    labels:
      opendatahub.io/dashboard: "true"
    name: mistral-7b-2x-t4
  spec:
    builtInAdapter:
      modelLoadingTimeoutMillis: 90000
    containers:
      - args:
          - --model
          - /mnt/models/
          - --download-dir
          - /models-cache
          - --port
          - "8080"
          - "--dtype"
          - float16
          - --tensor-parallel-size=2
        image: quay.io/rh-aiservices-bu/vllm-openai-ubi9:0.4.2
        name: kserve-container
        ports:
          - containerPort: 8080
            name: http1
            protocol: TCP
        resources:
          limits:
            cpu: "12"
            memory: 24Gi
          requests:
            cpu: "4"
            memory: 8Gi
        volumeMounts:
          - mountPath: /dev/shm
            name: shm
    multiModel: false
    supportedModelFormats:
      - autoSelect: true
        name: pytorch
    volumes:
      - emptyDir:
          medium: Memory
          sizeLimit: 15Gi
        name: shm
