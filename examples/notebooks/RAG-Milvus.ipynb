{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extending the capabilities of our model\n",
    "\n",
    "An LLM is a very capable tool, but only to the extent of the knowledge or information it has been trained on. After all, you only know what you know, right? But what if you need to ask a question that is not in the training data? Or what if you need to ask a question that is not in the training data, but is related to it?\n",
    "\n",
    "There are different ways to solve this problem, depending on the resources you have and the time or money you can spend on it. Here are a few options:\n",
    "\n",
    "- Fully retrain the model to include the information you need. For an LLM, it's only possible for a handful of companies in the world that can afford literally thousands of GPUs running for weeks.\n",
    "- Fine-tune the model with this new information. This requires way less resources, and can usually be done in a few hours or minutes (depending on the size of the model). However as it does not fully retrain the model, the new information may not be completely integrated in the answers. Fine-tuning excels at giving a better understanding of a specific context or vocabulary, a little bit less on injecting new knowledge. Plus you have to retrain and redeploy the model anyway any time you want to add more information.\n",
    "- Put this new information in a database and have the parts relevant to the query retrieved and added to this query as a context before sending it to the LLM. This technique is called **Retrieval Augmented Generation, or RAG**. It is interesting as you don't have to retrain or fine-tune the model to benefit of this new knowledge, that you can easily update at any time.\n",
    "\n",
    "We have already prepared a Vector Database using [Milvus](https://milvus.io/), where we have stored (in the form of [Embeddings](https://www.ibm.com/topics/embedding)) the content of the [California Driver's Handbook](https://www.dmv.ca.gov/portal/handbook/california-driver-handbook/).\n",
    "\n",
    "In this Notebook, we are going to use RAG to **make some queries about a Claim** and see how this new knowledge can help without having to modify our LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### Requirements and Imports\n",
    "\n",
    "If you have selected the right workbench image to launch as per the Lab's instructions, you should already have all the needed libraries. If not uncomment the first line in the next cell to install all the right packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e0eacb-a45c-47d0-b9e9-28d4fc790ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/app-root/lib/python3.9/site-packages (1.33.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/app-root/lib/python3.9/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/app-root/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/app-root/lib/python3.9/site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/app-root/lib/python3.9/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/app-root/lib/python3.9/site-packages (from openai) (1.10.15)\n",
      "Requirement already satisfied: tqdm>4 in /opt/app-root/lib/python3.9/site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: sniffio in /opt/app-root/lib/python3.9/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/app-root/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/app-root/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/app-root/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/app-root/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --no-cache-dir --no-dependencies --disable-pip-version-check -r requirements.txt # Uncomment only if you have not selected the right workbench image\n",
    "\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import LLMChain, RetrievalQA\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain_community.vectorstores import Milvus\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from milvus_retriever_with_score_threshold import MilvusRetrieverWithScoreThreshold\n",
    "\n",
    "#Turn off warnings when downloading the embedding model\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428fbad-2345-4536-b687-72416d6b9b15",
   "metadata": {},
   "source": [
    "### Langchain elements\n",
    "\n",
    "Again, we are going to use Langchain to define our task pipeline.\n",
    "\n",
    "First, the **LLM** where we will send our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM Inference Server URL\n",
    "inference_server_url = \"https://mistral-7b-vllm-mistral-7b-2x-t4.apps.cluster-78z4l.sandbox2699.opentlc.com\"\n",
    "\n",
    "# LLM definition\n",
    "llm = VLLMOpenAI(           # we are using the vLLM OpenAI-compatible API client. But the Model is running on OpenShift, not OpenAI.\n",
    "    openai_api_key=\"EMPTY\",   # and that is why we don't need an OpenAI key for this.\n",
    "    openai_api_base= f\"{inference_server_url}/v1\",\n",
    "    model_name=\"/mnt/models/\",\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa13907-14f1-4995-9756-8778c19a2101",
   "metadata": {},
   "source": [
    "Then the connection to the **vector database** where we have prepared and stored the California Driver Handbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba3d8f23-b9a4-4c33-aa77-caed4b523719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MILVUS_HOST = \"vectordb-milvus.milvus.svc.cluster.local\"\n",
    "MILVUS_PORT = 19530\n",
    "MILVUS_USERNAME = os.getenv('MILVUS_USERNAME')\n",
    "MILVUS_PASSWORD = os.getenv('MILVUS_PASSWORD')\n",
    "MILVUS_COLLECTION = os.getenv('MILVUS_COLLECTION_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f849c1a0-7fe5-425f-853d-6a9e67a38971",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 2.4.0.dev0, however, your version is 2.4.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "/opt/app-root/lib64/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# First we define the embeddings that we used to process the Handbook\n",
    "model_kwargs = {\"trust_remote_code\": True}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"nomic-ai/nomic-embed-text-v1\",\n",
    "            model_kwargs=model_kwargs,\n",
    "            show_progress=False,\n",
    "        )\n",
    "\n",
    "\n",
    "# Then we define the retriever that will fetch the relevant data from the Milvus vector store\n",
    "retriever = MilvusRetrieverWithScoreThreshold(\n",
    "            embedding_function=embeddings,\n",
    "            collection_name=MILVUS_COLLECTION,\n",
    "            collection_description=\"\",\n",
    "            collection_properties=None,\n",
    "            connection_args={\n",
    "                \"host\": MILVUS_HOST,\n",
    "                \"port\": MILVUS_PORT,\n",
    "                \"user\": MILVUS_USERNAME,\n",
    "                \"password\": MILVUS_PASSWORD,\n",
    "            },\n",
    "            consistency_level=\"Session\",\n",
    "            search_params=None,\n",
    "            k=4,\n",
    "            score_threshold=0.99,\n",
    "            metadata_field=\"metadata\",\n",
    "            text_field=\"page_content\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    "We will now define the **template** to use to make our query. Note that this template now contains a **References** section. That's were the documents returned from the vector database will be injected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8bb7517-faa2-43ed-a95d-835de975f916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template=\"\"\"<s>[INST]\n",
    "You are a helpful, respectful and honest assistant named \"Parasol Assistant\".\n",
    "You will be given a context, references to provide you with information, and a question.\n",
    "You must answer the question based as much as possible on this context.\n",
    "Always answer as helpfully as possible, while being safe.\n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "Context:\n",
    "{{context}}\n",
    "\n",
    "Question: {{question}} [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fbd67-220c-4a02-8e4e-7e0d1aa91588",
   "metadata": {},
   "source": [
    "We are now ready to query the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4cde40-3571-4e3c-9b05-78389765c98f",
   "metadata": {},
   "source": [
    "### First test, no additional knowledge\n",
    "\n",
    "Let's start with a first query about the claim, but without help from our vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3ac398b4-d555-45e5-aab9-d9b319f07108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a Jupyter Notebook in Red Hat OpenShift AI, you can follow these general steps:\n",
      "\n",
      "1. Log in to your Red Hat OpenShift cluster using the `oc` command-line tool or the web console.\n",
      "2. Create a new project if you don't have one already. You can do this by running the following command:\n",
      "   ```\n",
      "   oc new-project <project_name>\n",
      "   ```\n",
      "3. Create a new Jupyter Notebook application by running the following command:\n",
      "   ```\n",
      "   oc new-app quay.io/openshift-katas/jupyter-notebook~https://github.com/openshift/jupyter-notebook-example.git --name=<notebook_name>\n",
      "   ```\n",
      "   Replace `<project_name>` with the name of your project and `<notebook_name>` with the name you want to give to your Jupyter Notebook application.\n",
      "\n",
      "4. Once the application is created, you can access it by running the following command:\n",
      "   ```\n",
      "   oc expose svc/<notebook_name>-jupyter --type=NodePort\n",
      "   ```\n",
      "   This will expose the Jupyter Notebook application as a NodePort service. The port number will be printed in the output of the command.\n",
      "\n",
      "5. To access the Jupyter Notebook, you can use an SSH tunnel to forward the local port to the OpenShift cluster. Here's an example command:\n",
      "   ```\n",
      "   oc exec <notebook_name> -it -- /usr/bin/rhts sh -c 'echo \"c.c.localhost:<port_number>\" > /etc/hosts; jupyter notebook --no-browser --port=8888 --ip=0.0.0.0'\n",
      "   ```\n",
      "   Replace `<port_number>` with the port number that was printed when you ran the `oc expose` command earlier.\n",
      "\n",
      "6. Once the Jupyter Notebook is started, you can access it in your web browser by visiting `http://localhost:8888`.\n",
      "\n",
      "Please note that these instructions are just a general guideline, and the exact steps may vary depending on your specific setup and configuration. For more detailed instructions,"
     ]
    }
   ],
   "source": [
    "# Create and send our query.\n",
    "\n",
    "# query = \"Does Red Hat OpenShift AI support Habana Gaudi devices?\"\n",
    "# query = \"How can you work with GPU and taints in OpenShift AI?\"\n",
    "query = \"How can you create a Jupyter Notebook in Red Hat OpenShift AI?\"\n",
    "\n",
    "# Quick hack to reuse the same template with a different type of query.\n",
    "prompt_template = template.format(context=\"NO CONTEXT\", question=query)\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "conversation = LLMChain(\n",
    "            llm=llm,\n",
    "            prompt=prompt,\n",
    "            verbose=False\n",
    "        )\n",
    "resp = conversation.predict(context=\"\", question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0f714d-c6e7-4220-a16b-fc65dbae91fb",
   "metadata": {},
   "source": [
    "We can see that the answer is valid. Here the model is using its general understanding of traffic regulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4e9a93-9b81-424a-96a9-f447e417c8c1",
   "metadata": {},
   "source": [
    "### Second test, with added knowledge\n",
    "\n",
    "We will use the same prompt and query, but this time the model will have access to some references from the California's Driver Handbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dac009d5-d558-4258-9735-4fb0de46c309",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a Jupyter Notebook in Red Hat OpenShift AI, you need to follow these steps:\n",
      "\n",
      "1. Log in to Red Hat OpenShift AI and ensure that you are part of the required user or admin group for your OpenShift AI project.\n",
      "2. Create a data science project and a workbench using the Standard Data Science notebook image.\n",
      "3. Create and configure a pipeline server within the data science project that contains your workbench.\n",
      "4. Create and launch a Jupyter server from a notebook image that contains the Elyra extension (Standard data science, TensorFlow, TrustyAI, PyTorch, or HabanaAI). Make sure you have access to S3-compatible storage.\n",
      "5. After launching the Jupyter server, open JupyterLab and confirm that the JupyterLab launcher is automatically displayed. In the Elyra section of the JupyterLab launcher, click the Pipeline Editor tile to open it.\n",
      "\n",
      "You can now create and edit Jupyter Notebooks within the Pipeline Editor in JupyterLab."
     ]
    }
   ],
   "source": [
    "# Create and send our query.\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "            llm,\n",
    "            retriever=retriever,\n",
    "            chain_type_kwargs={\"prompt\": prompt},\n",
    "            return_source_documents=True,\n",
    "        )\n",
    "resp = rag_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5659f0-a27f-4b9e-8dd1-e05f37671c8f",
   "metadata": {},
   "source": [
    "That is pretty neat! Now the model refers directly to a source stating that **a red traffic signal light means \"STOP.\"**.\n",
    "\n",
    "But where did we get this information from? We can look into the sources associated with the answers from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8033f26e-e122-4708-b0bc-5be70f7bcaec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.9/html-single/working_on_data_science_projects/index 8\n",
      "https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.9/html-single/working_on_data_science_projects/index 65\n",
      "https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.9/html-single/getting_started_with_red_hat_openshift_ai_self-managed/index 44\n",
      "https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.9/html-single/managing_resources/index 2\n"
     ]
    }
   ],
   "source": [
    "for doc in resp['source_documents']:\n",
    "    if hasattr(doc, 'metadata') and isinstance(doc.metadata, dict) and 'source' in doc.metadata and 'page' in doc.metadata:\n",
    "        print(f'{doc.metadata[\"source\"]} {doc.metadata[\"page\"]}')\n",
    "    else:\n",
    "        print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf8cd32-0bdb-484d-a8bd-fb108ce2f131",
   "metadata": {},
   "source": [
    "That's it! We now know how to complement our LLM with some external knowledge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
